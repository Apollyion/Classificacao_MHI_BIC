{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58f25ba2fcce629d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T01:30:02.650340073Z",
     "start_time": "2023-09-26T01:30:02.633419151Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Contar indidencia de \"1\"s na coluna U\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Classificacao_WHOQOL/Datasets/df_base4.csv')\n",
    "\n",
    "\n",
    "target = df['U']\n",
    "df = df.drop(['U'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5146c8b30f817435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T01:52:13.697181145Z",
     "start_time": "2023-09-26T01:52:13.661491280Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Balancear os dados com undersampling\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# df, target = rus.fit_resample(df, target)\n",
    "\n",
    "X_train, X_, y_train, y_ = train_test_split(df, target,\n",
    "                                                    train_size=0.80, test_size=0.20)\n",
    "\n",
    "\n",
    "# Aplicando SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "X_test, X_cv, y_test, y_cv = train_test_split(X_, y_,\n",
    "                                                    train_size=0.60, test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77632b1530f80693",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier(verbosity=2, n_jobs=-1)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3621ba530c026f3d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotando "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26e82f5cc1fd6c17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T23:02:20.147146974Z",
     "start_time": "2023-09-25T23:02:20.100701666Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aplicando z-score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_cv = scaler.transform(X_cv)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5eecbe9d77439f9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T23:10:42.187022434Z",
     "start_time": "2023-09-25T23:10:25.341307477Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.556\n"
     ]
    }
   ],
   "source": [
    "# Aplicando SBS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "\n",
    "\n",
    "sbs = SFS(svm,\n",
    "          k_features=\"best\",\n",
    "          forward=True,\n",
    "          floating=False,\n",
    "          scoring='accuracy',\n",
    "          cv=0)\n",
    "\n",
    "sbs = sbs.fit(X_train, y_train)\n",
    "sbs.k_feature_names_\n",
    "\n",
    "# Pega os índices das colunas selecionadas e treina o modelo\n",
    "cols = list(sbs.k_feature_idx_)\n",
    "svm.fit(X_train[:, cols], y_train)\n",
    "y_pred = svm.predict(X_test[:, cols])\n",
    "print('Acurácia: %.3f' % accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a250b60901e87158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T23:04:08.207503882Z",
     "start_time": "2023-09-25T23:04:07.986726127Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=2, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=2, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando o modelo\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "rf.fit(X_train[:, cols], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f6697bfaa7f9081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T23:05:03.305521193Z",
     "start_time": "2023-09-25T23:05:03.251662210Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.596\n",
      "0.5959595959595959\n"
     ]
    }
   ],
   "source": [
    "# Avaliando o modelo\n",
    "y_pred = rf.predict(X_test[:, cols])\n",
    "print('Acurácia: %.3f' % accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c3679628f5cd4832",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T01:54:45.963222826Z",
     "start_time": "2023-09-26T01:54:45.915641126Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m dropout_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Dropout probability for regularization\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Split your data into training and validation sets\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Standardize your data\u001b[39;00m\n\u001b[1;32m     39\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m~/anaconda3/envs/bic/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2564\u001b[0m )\n\u001b[1;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bic/lib/python3.11/site-packages/sklearn/model_selection/_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2240\u001b[0m     )\n\u001b[1;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the neural network model with dropout for regularization\n",
    "class RobustClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(RobustClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 92  # Number of input features\n",
    "hidden_size = 128  # Number of units in the hidden layer\n",
    "output_size = 1  # Number of output units (1 for binary classification)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "dropout_prob = 0.5  # Dropout probability for regularization\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize your data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert your data to NumPy arrays and then to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(np.array(X_train))\n",
    "y_train_tensor = torch.Tensor(np.array(y_train).reshape(-1, 1))\n",
    "X_val_tensor = torch.Tensor(np.array(X_val))\n",
    "y_val_tensor = torch.Tensor(np.array(y_val).reshape(-1, 1))\n",
    "X_cv_tensor = torch.Tensor(np.array(X_cv))\n",
    "y_cv_tensor = torch.Tensor(np.array(y_cv).reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Create an instance of the model with dropout\n",
    "model = RobustClassifier(input_size, hidden_size, output_size, dropout_prob)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with early stopping based on validation loss\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    model.train()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validate the model on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
    "        predictions = model(torch.Tensor(X_cv_tensor))\n",
    "        predicted_classes = (predictions > 0.5).float().numpy()\n",
    "        accuracy = accuracy_score(y_cv_tensor, predicted_classes)\n",
    "        print(f'Accuracy on CV data: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Early stopping based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        print(\"Early stopping as validation loss is not improving.\")\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model(torch.Tensor(X_test))\n",
    "    predicted_classes = (predictions > 0.5).float().numpy()\n",
    "\n",
    "# Evaluate the model (e.g., calculate accuracy)\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "print(f'Accuracy on test data: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ff15bd9193aa6c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T01:29:22.279547267Z",
     "start_time": "2023-09-26T01:29:22.231202702Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82f5c4e96e06a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T01:46:57.664595100Z",
     "start_time": "2023-09-26T01:46:57.649635798Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CVSplit' from 'skorch.dataset' (/home/apo-pc/anaconda3/envs/bic/lib/python3.11/site-packages/skorch/dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeuralNetClassifier\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callback\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CVSplit\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Custom callback to track the best validation loss\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mValidLossCallback\u001b[39;00m(Callback):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CVSplit' from 'skorch.dataset' (/home/apo-pc/anaconda3/envs/bic/lib/python3.11/site-packages/skorch/dataset.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import Callback\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch.helper import EarlyStopping\n",
    "\n",
    "# Custom callback to track the best validation loss\n",
    "class ValidLossCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.best_valid_loss = np.inf\n",
    "\n",
    "    def on_epoch_end(self, net, **kwargs):\n",
    "        valid_loss = net.history[-1, 'valid_loss']\n",
    "        if valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = valid_loss\n",
    "            net.best_valid_loss = valid_loss\n",
    "\n",
    "# Define the neural network model with dropout for regularization\n",
    "class RobustClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(RobustClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 92  # Number of input features\n",
    "output_size = 1  # Number of output units (1 for binary classification)\n",
    "dropout_prob = 0.5  # Dropout probability for regularization\n",
    "\n",
    "# Load your data (X_train, y_train, X_cv, y_cv, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# Standardize your data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_cv = scaler.transform(X_cv)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.Tensor(np.array(X_train))\n",
    "y_train_tensor = torch.Tensor(np.array(y_train).reshape(-1, 1))\n",
    "X_cv_tensor = torch.Tensor(np.array(X_cv))\n",
    "y_cv_tensor = torch.Tensor(np.array(y_cv).reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Create an instance of the custom model with dropout\n",
    "model = NeuralNetClassifier(\n",
    "    RobustClassifier,\n",
    "    module__input_size=input_size,\n",
    "    module__hidden_size=64,  # Initial hidden size\n",
    "    module__output_size=output_size,\n",
    "    module__dropout_prob=dropout_prob,\n",
    "    optimizer=optim.Adam,\n",
    "    optimizer__lr=0.001,\n",
    "    criterion=nn.BCELoss,\n",
    "    callbacks=[('valid_loss', ValidLossCallback()), ('early_stop', EarlyStopping(patience=10))],\n",
    "    max_epochs=100,\n",
    "    batch_size=64,\n",
    "    train_split=CVSplit(stratified=True, random_state=42),  # Enable train/validation split\n",
    ")\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'module__hidden_size': [64, 128, 256],\n",
    "    'optimizer__lr': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Use cross-validation for hyperparameter tuning\n",
    "cv_splits = 5  # Number of cross-validation folds\n",
    "skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object for hyperparameter tuning with skorch\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=skf, verbose=2)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_hidden_size = best_params['module__hidden_size']\n",
    "best_learning_rate = best_params['optimizer__lr']\n",
    "\n",
    "print(f'Best hyperparameters: Hidden Size = {best_hidden_size}, Learning Rate = {best_learning_rate}')\n",
    "\n",
    "# Create a new instance of the custom model with the best hyperparameters\n",
    "best_model = NeuralNetClassifier(\n",
    "    RobustClassifier,\n",
    "    module__input_size=input_size,\n",
    "    module__hidden_size=best_hidden_size,\n",
    "    module__output_size=output_size,\n",
    "    module__dropout_prob=dropout_prob,\n",
    "    optimizer=optim.Adam,\n",
    "    optimizer__lr=best_learning_rate,\n",
    "    criterion=nn.BCELoss,\n",
    "    callbacks=[('valid_loss', ValidLossCallback()), ('early_stop', EarlyStopping(patience=10))],\n",
    "    max_epochs=100,\n",
    "    batch_size=64,\n",
    "    train_split=CVSplit(stratified=True, random_state=42),  # Enable train/validation split\n",
    ")\n",
    "\n",
    "# Fit the best model to the training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "predicted_classes = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model (e.g., calculate accuracy)\n",
    "accuracy = accuracy_score(y_test, predicted_classes)\n",
    "print(f'Accuracy on test data: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4019d558bf8839a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-26T01:40:07.737708965Z",
     "start_time": "2023-09-26T01:40:07.721748204Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train\u001b[38;5;241m.\u001b[39mtype()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "X_train.type()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
