{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunning dos hiperparâmetros\n",
    "- Será realizado o tunning para os seguintes casos:\n",
    "    1) sem informações do WHOQOL na base\n",
    "    2) com todas as respostas do WHOQOL\n",
    "    3) só com o índice final do WHOQOL\n",
    "    4) só com os índices dos domínios do WHOQOL\n",
    "\n",
    "- Para os seguintes modelos:\n",
    "    1) Random Forest\n",
    "    2) SVM\n",
    "    3) MLP\n",
    "    4) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow:\n",
    "Para cada caso:\n",
    "- Importar dados\n",
    "- Separar em treino e teste ( Preciso\n",
    "- Definir os parâmetros a serem testados\n",
    "- Realizar o tunning\n",
    "- Salvar os resultados graficamente\n",
    "- Salvar o melhor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\anaconda3\\envs\\BIC\\Lib\\site-packages\\tpot\\builtins\\__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo a função"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T00:14:10.473676900Z",
     "start_time": "2023-06-17T00:14:10.364216200Z"
    }
   },
   "outputs": [],
   "source": [
    "def perform_model_tuning(data, target, models, param_grids, base_name):\n",
    "    # Realizar Random Under-Sampling\n",
    "    sampler = RandomUnderSampler()\n",
    "    data_resampled, target_resampled = sampler.fit_resample(data, target)\n",
    "\n",
    "    # Dividir os dados em treinamento e teste após o Random Under-Sampling\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_resampled, target_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "    best_models = {}  # Dicionário para armazenar o melhor modelo de cada tipo\n",
    "    model_accuracies = {}  # Dicionário para armazenar as acurácias de cada modelo\n",
    "    current_dir = os.getcwd()\n",
    "    for model_name, model, param_grid_entry in zip(models.keys(), models.values(), param_grids):\n",
    "        print(f\"Tuning do modelo {model_name} iniciado...\")\n",
    "\n",
    "        # Acessar o dicionário de parâmetros correspondente ao modelo atual\n",
    "        param_grid = param_grid_entry['params']\n",
    "\n",
    "        # Realizar a busca em grid para encontrar os melhores parâmetros\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=10, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Melhor modelo encontrado\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Fazer previsões no conjunto de teste\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Calcular a acurácia do modelo\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        model_accuracies[model_name] = accuracy\n",
    "\n",
    "        print(f\"Acurácia do modelo {model_name}: {accuracy:.4f}\")\n",
    "\n",
    "        # Salvar o melhor modelo\n",
    "        best_models[model_name] = best_model\n",
    "\n",
    "    # Plotar gráfico de desempenho comparativo\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_accuracies.keys(), model_accuracies.values())\n",
    "    plt.xlabel('Modelos')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.title('Comparação de desempenho dos modelos')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    graph_filename = f\"{base_name}_graph_{model_name}.png\"\n",
    "    graph_path = os.path.join(current_dir, graph_filename)\n",
    "    plt.savefig(graph_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Salvar tabela com os valores de acurácia\n",
    "    table_filename = f\"{base_name}_table_{model_name}.csv\"\n",
    "    accuracies_df = pd.DataFrame.from_dict(model_accuracies, orient='index', columns=['Acurácia'])\n",
    "    table_path = os.path.join(current_dir, table_filename)\n",
    "    accuracies_df.to_csv(table_path, index=False)\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T00:14:11.041692700Z",
     "start_time": "2023-06-17T00:14:10.900473100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sem WHOQOL:\n",
    "base1 = pd.read_csv('Datasets/dataframe_socioeconomico_Niveis.csv')\n",
    "data1 = base1.drop('Nivel_MHI', axis=1)\n",
    "target1 = base1['Nivel_MHI']\n",
    "le = LabelEncoder()\n",
    "target1 = le.fit_transform(target1)\n",
    "\n",
    "# com todas as respostas do WHOQOL\n",
    "base2 = pd.read_csv('Datasets/df_social_whoqol_tratado_1.csv')\n",
    "data2 = base2.drop('Nivel_MHI', axis=1)\n",
    "target2 = base2['Nivel_MHI']\n",
    "target2 = le.fit_transform(target2)\n",
    "\n",
    "\n",
    "# só com o indice dos dominiosdo WHOQOL, concatenar base1 com final da base2\n",
    "data3 = pd.concat([data1, base2.loc[:,['CR','CS','CT','CU']]], axis=1)\n",
    "target3 = base2['Nivel_MHI']\n",
    "target3 = le.fit_transform(target3)\n",
    "\n",
    "\n",
    "# Só com a qualidade de vida do WHOQOL\n",
    "data4 = pd.concat([data1, base2['BR']], axis=1)\n",
    "target4 = base2['Nivel_MHI']\n",
    "target4 = le.fit_transform(target4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T00:14:11.529746Z",
     "start_time": "2023-06-17T00:14:11.506595100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definindo parametros:\n",
    "# Definir os modelos e os grids de parâmetros\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'MLP': MLPClassifier(),\n",
    "    'XGBoost': XGBClassifier()\n",
    "}\n",
    "\n",
    "param_grids = [\n",
    "    {'model': 'Random Forest', 'params': {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}},\n",
    "    {'model': 'SVM', 'params': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}},\n",
    "    {'model': 'MLP', 'params': {'hidden_layer_sizes': [(10,), (50,), (100,)], 'alpha': [0.001, 0.01, 0.1]}},\n",
    "    {'model': 'XGBoost', 'params': {'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 7]}},\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T00:16:51.335921700Z",
     "start_time": "2023-06-17T00:16:42.708674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67045455 0.67045455 0.66666667 0.67816092 0.64367816 0.65517241\n",
      " 0.64367816 0.63218391 0.66666667 0.66666667]\n",
      "0.6593782654127482\n"
     ]
    }
   ],
   "source": [
    "# Chamando função para cada uma das bases\n",
    "# Aplicando Random Forest no under-sampling com Cross Validation de 10 folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = RandomForestClassifier()\n",
    "c1= cross_val_score(model, data1, target1, cv=10)\n",
    "print(c1)\n",
    "print(c1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T00:16:57.370418100Z",
     "start_time": "2023-06-17T00:16:51.335921700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76136364 0.71590909 0.82758621 0.8045977  0.79310345 0.7816092\n",
      " 0.77011494 0.85057471 0.73563218 0.81609195]\n",
      "0.7856583072100313\n"
     ]
    }
   ],
   "source": [
    "c2 = cross_val_score(model, data2, target2, cv=10)\n",
    "print(c2)\n",
    "print(c2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T00:17:03.629682Z",
     "start_time": "2023-06-17T00:16:57.370418100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72727273 0.72727273 0.79310345 0.72413793 0.79310345 0.68965517\n",
      " 0.72413793 0.82758621 0.74712644 0.73563218]\n",
      "0.7489028213166143\n"
     ]
    }
   ],
   "source": [
    "c3 = cross_val_score(model, data3, target3, cv=10)\n",
    "print(c3)\n",
    "print(c3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-17T00:17:09.541388900Z",
     "start_time": "2023-06-17T00:17:03.643189100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67045455 0.65909091 0.66666667 0.68965517 0.65517241 0.71264368\n",
      " 0.63218391 0.6091954  0.64367816 0.67816092]\n",
      "0.6616901776384535\n"
     ]
    }
   ],
   "source": [
    "c4 = cross_val_score(model, data4, target4, cv=10)\n",
    "print(c4)\n",
    "print(c4.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamando função para cada uma das bases\n",
    "print(\"========================== Data 1  =================================\")\n",
    "best_models1 = perform_model_tuning(data1, target1, models, param_grids, \"data1\")\n",
    "print(\"========================== Data 2  =================================\")\n",
    "best_models2 = perform_model_tuning(data2, target2, models, param_grids,\"data2\")\n",
    "print(\"========================== Data 3  =================================\")\n",
    "best_models3 = perform_model_tuning(data3, target3, models, param_grids, \"data3\")\n",
    "print(\"========================== Data 4  =================================\")\n",
    "best_models4 = perform_model_tuning(data4, target4, models, param_grids, \"data4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"========================== Data 1  =================================\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1, target1, test_size=0.2)\n",
    "tpot = TPOTClassifier(verbosity=2, config_dict='TPOT light')\n",
    "tpot.fit(X_train, y_train)\n",
    "accuracy = tpot.score(X_test, y_test)\n",
    "print(f\"Acurácia do melhor modelo encontrado: {accuracy}\")\n",
    "tpot.export('tpot_best_model1.py')\n",
    "print(\"========================== Data 2  =================================\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data2, target2, test_size=0.2)\n",
    "tpot2 = TPOTClassifier(verbosity=2, config_dict='TPOT light')\n",
    "tpot2.fit(X_train, y_train)\n",
    "accuracy = tpot2.score(X_test, y_test)\n",
    "print(f\"Acurácia do melhor modelo encontrado: {accuracy}\")\n",
    "tpot2.export('tpot_best_model2.py')\n",
    "print(\"========================== Data 3  =================================\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data3, target3, test_size=0.2)\n",
    "tpot3 = TPOTClassifier(verbosity=2, config_dict='TPOT light')\n",
    "tpot3.fit(X_train, y_train)\n",
    "accuracy = tpot3.score(X_test, y_test)\n",
    "print(f\"Acurácia do melhor modelo encontrado: {accuracy}\")\n",
    "tpot3.export('tpot_best_model3.py')\n",
    "print(\"========================== Data 4  =================================\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(data4, target4, test_size=0.2)\n",
    "tpot4 = TPOTClassifier(verbosity=2,config_dict='TPOT light')\n",
    "tpot4.fit(X_train, y_train)\n",
    "accuracy = tpot4.score(X_test, y_test)\n",
    "print(f\"Acurácia do melhor modelo encontrado: {accuracy}\")\n",
    "tpot4.export('tpot_best_model4.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
